# Diffusion Models Distillation Papers
Papers and projects of distillation for diffusion models

## Overview Materials
- [Awesome Knowledge Distillation Papers](https://github.com/dkozlov/awesome-knowledge-distillation)


## Selected Papers

- **Knowledge distillation in iterative generative models for improved sampling speed**. 2021.

    *Eric Luhman, Troy Luhman*. [[pdf](https://arxiv.org/abs/2101.02388)]

- **Progressive Distillation for Fast Sampling of Diffusion Models**. ICLR 2022.

    *Tim Salimans and Jonathan Ho*. [[pdf](https://arxiv.org/abs/2202.00512)]

- **On Distillation of Guided Diffusion Models**. CVPR 2023.

    *Chenlin Meng and Robin Rombach and Ruiqi Gao and Diederik P. Kingma and Stefano Ermon and Jonathan Ho and Tim Salimans*. [[pdf](https://arxiv.org/abs/2210.03142)]

- **TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation**. 2023

    *Berthelot, David and Autef, Arnaud and Lin, Jierui and Yap, Dian Ang and Zhai, Shuangfei and Hu, Siyuan and Zheng, Daniel and Talbott, Walter and Gu, Eric*. [[pdf]](https://arxiv.org/abs/2303.04248)

- **BK-SDM: Architecturally Compressed Stable Diffusion for Efficient Text-to-Image Generation**. ICML 2023.

    *Kim, Bo-Kyeong and Song, Hyoung-Kyu and Castells, Thibault and Choi, Shinkook*. [[pdf](https://openreview.net/forum?id=bOVydU0XKC)]

- **On Architectural Compression of Text-to-Image Diffusion Models**. 2023.

    *Kim, Bo-Kyeong and Song, Hyoung-Kyu and Castells, Thibault and Choi, Shinkook*. [[pdf](https://arxiv.org/abs/2305.15798)]

- **Knowledge Diffusion for Distillation**. 2023.

    *Tao Huang, Yuan Zhang, Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Chang Xu* [[pdf](https://www.researchgate.net/publication/371040763_Knowledge_Diffusion_for_Distillation)]

- **SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds**. 2023.

    *Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, Jian Ren1*. [[pdf](https://snap-research.github.io/SnapFusion/)]

- **BOOT: Data-free Distillation of Denoising Diffusion Models with Bootstrapping**. 2023. 

    *Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, Josh Susskind*. [[pdf](https://arxiv.org/abs/2306.05544)]

- **Consistency models**. 2023

    *Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever*. [[pdf](https://arxiv.org/abs/2303.01469)]


## Projects
- [diffusion_models_distillation](https://github.com/YongfeiYan/diffusion_models_distillation)
- [small-stable-diffusion-v0](https://huggingface.co/OFA-Sys/small-stable-diffusion-v0#small-stable-diffusion-model-card)
- [Progressive Distillation](https://github.com/google-research/google-research/tree/master/diffusion_distillation)
- [distill-sd](https://github.com/segmind/distill-sd)
- [ml-tract](https://github.com/apple/ml-tract/tree/main)
- [Diff-Pruning](https://github.com/VainF/Diff-Pruning)
- [diffusion_distiller](https://github.com/Hramchenko/diffusion_distiller)
- [Denoising_Student](https://github.com/tcl9876/Denoising_Student)
